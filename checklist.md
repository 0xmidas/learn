# AI/PyTorch Learning Checklist

## Fundamentals
- [x] MLP
- [x] ReLU activation
- [x] Training loop
- [x] Evaluation loop

## CNN Fundamentals
- [x] Conv2d implementation (naive nested loops)
- [x] MaxPool2d implementation
- [x] Basic CNN architecture (conv → relu → pool)
- [x] Train on MNIST
- [x] im2col efficient convolution
- [x] Batch normalization
- [x] Residual/skip connections
- [ ] Depthwise separable convolutions
- [ ] Transposed convolutions

## Classic Architectures
- [x] Simple ConvNet (LeNet-style)
- [x] ResNet
- [ ] Inception

## Sequence Models
- [x] RNN
- [x] LSTM
- [ ] GRU

## Attention & Transformers
- [ ] Attention mechanism
- [ ] Self-attention
- [ ] Transformer encoder
- [ ] Transformer decoder
- [ ] Vision Transformer (ViT)

## Generative Models
- [ ] Autoencoder
- [ ] Variational Autoencoder (VAE)
- [ ] GAN
- [ ] Diffusion

## Deep RL
- [ ] Q-learning (tabular)
- [ ] DQN
- [ ] Policy Gradient / REINFORCE
- [ ] Actor-Critic / PPO

## NLP Fundamentals
- [ ] Word embeddings (Word2Vec / skip-gram)
- [ ] Contrastive learning (CLIP-style)
